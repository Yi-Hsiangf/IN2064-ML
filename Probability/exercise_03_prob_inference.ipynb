{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Task: Probabilistic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import loggamma\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task\n",
    "This notebook contains code implementing the methods discussed in `Lecture 3: Probabilistic Inference`. Some functions in this notebook are incomplete. Your task is to fill in the missing code and run the entire notebook. \n",
    "\n",
    "In the beginning of every function there is docstring which specifies the input and and expected output.\n",
    "Write your code in a way that adheres to it.\n",
    "You may only use plain python and anything that we imported for you above such as `numpy` functions (i.e. no scikit-learn classifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the results to PDF\n",
    "Once you complete the assignments, export the entire notebook as PDF and attach it to your homework solutions. \n",
    "The best way of doing that is\n",
    "1. Run all the cells of the notebook (`Kernel -> Restart & Run All`)\n",
    "2. Export/download the notebook as PDF (`File -> Download as -> PDF via LaTeX (.pdf)`)\n",
    "3. Concatenate your solutions for other tasks with the output of Step 2. On Linux you can simply use `pdfunite`, there are similar tools for other platforms too. You can only upload a single PDF file to Moodle.\n",
    "\n",
    "**Make sure** you are using `nbconvert` **Version 5.5 or later** by running `jupyter nbconvert --version`. Older versions clip lines that exceed page width, which makes your code harder to grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating data\n",
    "The following function simulates flipping a biased coin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is given, nothing to do here.\n",
    "def simulate_data(num_samples, tails_proba):\n",
    "    \"\"\"Simulate a sequence of i.i.d. coin flips.\n",
    "    \n",
    "    Tails are denoted as 1 and heads are denoted as 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_samples : int\n",
    "        Number of samples to generate.\n",
    "    tails_proba : float in range (0, 1)\n",
    "        Probability of observing tails.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    samples : array, shape (num_samples)\n",
    "        Outcomes of simulated coin flips. Tails is 1 and heads is 0.\n",
    "    \"\"\"\n",
    "    return np.random.choice([0, 1], size=(num_samples), p=[1 - tails_proba, tails_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)  # for reproducibility\n",
    "num_samples = 20\n",
    "tails_proba = 0.7\n",
    "samples = simulate_data(num_samples, tails_proba)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important: Numerical stability\n",
    "When dealing with probabilities, we often encounter extremely small numbers. Because of limited floating point precision, directly manipulating such small numbers can lead to serious numerical issues, such as overflows and underflows. Therefore, we usually work in the **log-space**.\n",
    "\n",
    "For example, if we want to multiply two tiny numbers $a$ and $b$, we should compute $\\exp(\\log(a) + \\log(b))$ instead of naively multiplying $a \\cdot b$.\n",
    "\n",
    "For this reason, we usually compute **log-probabilities** instead of **probabilities**. Virtually all machine learning libraries are dealing with log-probabilities instead of probabilities (e.g. [Tensorflow-probability](https://www.tensorflow.org/probability) or [Pyro](https://pyro.ai))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Compute $\\log p(\\mathcal{D} \\mid \\theta)$ for different values of $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(theta, samples):\n",
    "    \"\"\"Compute log p(D | theta) for the given values of theta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array, shape (num_points)\n",
    "        Values of theta for which it's necessary to evaluate the log-likelihood.\n",
    "    samples : array, shape (num_samples)\n",
    "        Outcomes of simulated coin flips. Tails is 1 and heads is 0.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_likelihood : array, shape (num_points)\n",
    "        Values of log-likelihood for each value in theta.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1e-5, 1-1e-5, 1000)\n",
    "log_likelihood = compute_log_likelihood(x, samples)\n",
    "likelihood = np.exp(log_likelihood)\n",
    "plt.plot(x, likelihood, label='likelihood', c='purple')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the likelihood function doesn't define a probability distribution over $\\theta$ --- the integral $\\int_{0}^{1} p(\\mathcal{D} \\mid \\theta) d\\theta$ is not equal to one.\n",
    "\n",
    "To show this, we approximate $\\int_{0}^{1} p(\\mathcal{D} \\mid \\theta) d\\theta$ numerically using [the rectangle rule](https://en.wikipedia.org/wiki/Riemann_sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 is the length of the interval over which we are integrating p(D | theta)\n",
    "int_likelihood = 1.0 * np.mean(likelihood)\n",
    "print(f'Integral = {int_likelihood:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Compute $\\log p(\\theta \\mid a, b)$ for different values of $\\theta$\n",
    "The function `loggamma` from the `scipy.special` package might be useful here. (It's already imported - see the first cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prior(theta, a, b):\n",
    "    \"\"\"Compute log p(theta | a, b) for the given values of theta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array, shape (num_points)\n",
    "        Values of theta for which it's necessary to evaluate the log-prior.\n",
    "    a, b: float\n",
    "        Parameters of the prior Beta distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_prior : array, shape (num_points)\n",
    "        Values of log-prior for each value in theta.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1e-5, 1-1e-5, 1000)\n",
    "a, b = 3, 5\n",
    "\n",
    "# Plot the prior distribution\n",
    "log_prior = compute_log_prior(x, a, b)\n",
    "prior = np.exp(log_prior)\n",
    "plt.plot(x, prior, label='prior')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the likelihood, the prior defines a probability distribution over $\\theta$ and integrates to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "int_prior = 1.0 * np.mean(prior)\n",
    "print(f'Integral = {int_prior:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Compute $\\log p(\\theta \\mid \\mathcal{D}, a, b)$ for different values of $\\theta$\n",
    "The function `loggamma` from the `scipy.special` package might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_posterior(theta, samples, a, b):\n",
    "    \"\"\"Compute log p(theta | D, a, b) for the given values of theta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array, shape (num_points)\n",
    "        Values of theta for which it's necessary to evaluate the log-prior.\n",
    "    samples : array, shape (num_samples)\n",
    "        Outcomes of simulated coin flips. Tails is 1 and heads is 0.\n",
    "    a, b: float\n",
    "        Parameters of the prior Beta distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_posterior : array, shape (num_points)\n",
    "        Values of log-posterior for each value in theta.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1e-5, 1-1e-5, 1000)\n",
    "\n",
    "log_posterior = compute_log_posterior(x, samples, a, b)\n",
    "posterior = np.exp(log_posterior)\n",
    "plt.plot(x, posterior, label='posterior', c='orange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the prior, the posterior defines a probability distribution over $\\theta$ and integrates to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_posterior = 1.0 * np.mean(posterior)\n",
    "print(f'Integral = {int_posterior:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Compute $\\theta_{MLE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_mle(samples):\n",
    "    \"\"\"Compute theta_MLE for the given data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : array, shape (num_samples)\n",
    "        Outcomes of simulated coin flips. Tails is 1 and heads is 0.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta_mle : float\n",
    "        Maximum likelihood estimate of theta.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_mle = compute_theta_mle(samples)\n",
    "print(f'theta_mle = {theta_mle:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Compute $\\theta_{MAP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_map(samples, a, b):\n",
    "    \"\"\"Compute theta_MAP for the given data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : array, shape (num_samples)\n",
    "        Outcomes of simulated coin flips. Tails is 1 and heads is 0.\n",
    "    a, b: float\n",
    "        Parameters of the prior Beta distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta_mle : float\n",
    "        Maximum a posteriori estimate of theta.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_map = compute_theta_map(samples, a, b)\n",
    "print(f'theta_map = {theta_map:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play around with the values of `a`, `b`, `num_samples` and `tails_proba` to see how the results are changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "tails_proba = 0.7\n",
    "samples = simulate_data(num_samples, tails_proba)\n",
    "a, b = 3, 5\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 8])\n",
    "x = np.linspace(1e-5, 1-1e-5, 1000)\n",
    "\n",
    "# Plot the prior distribution\n",
    "log_prior = compute_log_prior(x, a, b)\n",
    "prior = np.exp(log_prior)\n",
    "plt.plot(x, prior, label='prior')\n",
    "\n",
    "# Plot the likelihood\n",
    "log_likelihood = compute_log_likelihood(x, samples)\n",
    "likelihood = np.exp(log_likelihood)\n",
    "int_likelihood = np.mean(likelihood)\n",
    "# We rescale the likelihood - otherwise it would be impossible to see in the plot\n",
    "rescaled_likelihood = likelihood / int_likelihood\n",
    "plt.plot(x, rescaled_likelihood, label='scaled likelihood', color='purple')\n",
    "\n",
    "# Plot the posterior distribution\n",
    "log_posterior = compute_log_posterior(x, samples, a, b)\n",
    "posterior = np.exp(log_posterior)\n",
    "plt.plot(x, posterior, label='posterior')\n",
    "\n",
    "# Visualize theta_mle\n",
    "theta_mle = compute_theta_mle(samples)\n",
    "ymax = np.exp(compute_log_likelihood(np.array([theta_mle]), samples)) / int_likelihood\n",
    "plt.vlines(x=theta_mle, ymin=0.00, ymax=ymax, linestyle='dashed', color='purple', label=r'$\\theta_{MLE}$')\n",
    "\n",
    "\n",
    "# Visualize theta_map\n",
    "theta_map = compute_theta_map(samples, a, b)\n",
    "ymax = np.exp(compute_log_posterior(np.array([theta_map]), samples, a, b))\n",
    "plt.vlines(x=theta_map, ymin=0.00, ymax=ymax, linestyle='dashed', color='orange', label=r'$\\theta_{MAP}$')\n",
    "\n",
    "plt.xlabel(r'$\\theta$', fontsize='xx-large')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "207px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}